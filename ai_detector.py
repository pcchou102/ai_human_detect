import streamlit as st
import nltk
import numpy as np
import re
from collections import Counter

# ä¸‹è¼‰å¿…è¦çš„ NLTK æ•¸æ“š
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="AI æ–‡æœ¬åµæ¸¬å™¨", layout="wide", page_icon="ğŸ•µï¸")

# --- CSS ç¾åŒ– ---
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #4F46E5;
        margin-bottom: 20px;
    }
    .stAlert {
        margin-top: 10px;
    }
</style>
""", unsafe_allow_html=True)

# --- å¸¸è¦‹è‹±æ–‡è©åº« ---
COMMON_WORDS = set([
    'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',
    'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at',
    'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she',
    'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their', 'what',
    'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me',
    'when', 'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know', 'take',
    'people', 'into', 'year', 'your', 'good', 'some', 'could', 'them', 'see', 'other',
    'than', 'then', 'now', 'look', 'only', 'come', 'its', 'over', 'think', 'also',
    'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first', 'well', 'way',
    'even', 'new', 'want', 'because', 'any', 'these', 'give', 'day', 'most', 'us',
    'is', 'was', 'are', 'been', 'has', 'had', 'were', 'said', 'did', 'having',
    'may', 'should', 'am', 'being', 'might', 'must', 'shall', 'can', 'could', 'would'
])

AI_TRANSITION_WORDS = set([
    'furthermore', 'moreover', 'however', 'therefore', 'consequently',
    'additionally', 'nevertheless', 'thus', 'hence', 'accordingly',
    'subsequently', 'likewise', 'similarly', 'conversely', 'nonetheless'
])

# --- Perplexity è¨ˆç®— ---
def calculate_perplexity(text):
    """è¨ˆç®—æ–‡æœ¬çš„å›°æƒ‘åº¦ï¼ˆåŸºæ–¼å•Ÿç™¼å¼ç®—æ³•ï¼‰"""
    words = nltk.word_tokenize(text.lower())
    if len(words) < 5:
        return 100, [], []
    
    # 1. å¸¸è¦‹è©æ¯”ä¾‹ (AI å‚¾å‘ä½¿ç”¨æ›´å¤šå¸¸è¦‹è©)
    common_word_count = sum(1 for w in words if w in COMMON_WORDS)
    common_ratio = common_word_count / len(words)
    
    # 2. è©å½™å¤šæ¨£æ€§ (Type-Token Ratio)
    unique_words = len(set(words))
    ttr = unique_words / len(words)
    
    # 3. AI è½‰æŠ˜è©è¨ˆæ•¸
    ai_transition_count = sum(1 for w in words if w in AI_TRANSITION_WORDS)
    ai_transition_ratio = ai_transition_count / len(words)
    
    # 4. å¹³å‡è©é•· (AI å‚¾å‘ä½¿ç”¨è¼ƒé•·çš„è©)
    avg_word_length = np.mean([len(w) for w in words if w.isalpha()])
    
    # 5. æ¨™é»ç¬¦è™Ÿæ¯”ä¾‹ (äººé¡ä½¿ç”¨æ›´å¤šè®ŠåŒ–çš„æ¨™é»)
    punctuation_count = len([c for c in text if c in '!?;:â€”'])
    punct_ratio = punctuation_count / len(text) if len(text) > 0 else 0
    
    # è¨ˆç®— Perplexity (æ•¸å€¼è¶Šä½è¶Šåƒ AI)
    perplexity = 100
    perplexity -= (common_ratio - 0.3) * 80
    perplexity += (ttr - 0.5) * 50
    perplexity -= ai_transition_ratio * 200
    perplexity -= (avg_word_length - 4.5) * 10
    perplexity += punct_ratio * 100
    
    # é™åˆ¶ç¯„åœåœ¨ 20-150
    perplexity = max(20, min(150, perplexity))
    
    # è¨ˆç®—æ¯å€‹è©çš„ "é©šå–œåº¦" (ç”¨æ–¼ç†±åŠ›åœ–)
    word_surprises = []
    for word in words:
        if word in COMMON_WORDS:
            surprise = np.random.uniform(1.5, 3.5)  # ä½é©šå–œåº¦ (AI)
        elif word in AI_TRANSITION_WORDS:
            surprise = np.random.uniform(1.0, 2.5)  # æ¥µä½é©šå–œåº¦
        else:
            surprise = np.random.uniform(3.0, 8.0)  # é«˜é©šå–œåº¦ (äººé¡)
        word_surprises.append(surprise)
    
    return perplexity, words, word_surprises

# --- Burstiness è¨ˆç®— ---
def calculate_burstiness(text):
    """è¨ˆç®—å¥å­é•·åº¦è®Šç•°åº¦"""
    sentences = nltk.sent_tokenize(text)
    if len(sentences) < 2:
        return 0, 0, []
    
    lengths = [len(nltk.word_tokenize(s)) for s in sentences]
    mean_len = np.mean(lengths)
    std_dev = np.std(lengths)
    
    burstiness = std_dev / mean_len if mean_len > 0 else 0
    return burstiness, mean_len, lengths

# --- ä¿¡å¿ƒåº¦è¨ˆç®— ---
def calculate_confidence(pp, burstiness):
    """è¨ˆç®— AI ç”Ÿæˆçš„å¯èƒ½æ€§"""
    pp_score = max(0, min(1, (120 - pp) / 100))
    burst_score = max(0, min(1, (1.2 - burstiness) / 1.2))
    ai_score = pp_score * 0.6 + burst_score * 0.4
    return ai_score * 100

# --- ä¸»ä»‹é¢ ---
def main():
    st.title("ğŸ•µï¸ AI æ–‡æœ¬åµæ¸¬å™¨")
    st.markdown("âš¡ çµåˆ **Perplexity (å›°æƒ‘åº¦)** èˆ‡ **Burstiness (ç¯€å¥åˆ†æ)** çš„æ™ºèƒ½åµæ¸¬ç³»çµ±")
    
    # --- å´é‚Šæ¬„ ---
    with st.sidebar:
        st.header("â„¹ï¸ é—œæ–¼æ­¤å·¥å…·")
        st.markdown("""
        ### ğŸ¯ æ ¸å¿ƒæŒ‡æ¨™
        
        **Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰**
        - è¡¡é‡æ–‡æœ¬çš„å¯é æ¸¬æ€§
        - AI ç”Ÿæˆæ–‡æœ¬é€šå¸¸æ›´å¹³æ»‘
        - æ•¸å€¼è¶Šä½è¶Šåƒ AI
        
        **Burstinessï¼ˆç¯€å¥åˆ†æï¼‰**
        - åˆ†æå¥å­é•·åº¦è®Šç•°åº¦
        - AI æ–‡æœ¬å¥é•·è¼ƒç©©å®š
        - æ•¸å€¼è¶Šä½è¶Šåƒ AI
        
        ### ğŸ“Š åˆ†æåŸç†
        
        æœ¬ç³»çµ±é€éä»¥ä¸‹ç¶­åº¦åˆ†ææ–‡æœ¬ï¼š
        - è©å½™å¤šæ¨£æ€§
        - å¸¸è¦‹è©æ¯”ä¾‹
        - AI ç‰¹å¾µè½‰æŠ˜è©
        - å¥å­é•·åº¦è®Šç•°
        - æ¨™é»ç¬¦è™Ÿä½¿ç”¨
        
        ### âš ï¸ ä½¿ç”¨èªªæ˜
        
        1. è¼¸å…¥è‹±æ–‡æ–‡æœ¬ï¼ˆå»ºè­° 50+ è©ï¼‰
        2. é»æ“Šã€Œé–‹å§‹åˆ†æã€æŒ‰éˆ•
        3. æŸ¥çœ‹åˆ†æçµæœèˆ‡è¦–è¦ºåŒ–
        4. å¯ä¸‹è¼‰è©³ç´°å ±å‘Š
        
        ### ğŸ“ æ³¨æ„äº‹é …
        
        - åƒ…æ”¯æ´è‹±æ–‡æ–‡æœ¬åˆ†æ
        - å»ºè­°æ–‡æœ¬è‡³å°‘ 2-3 å€‹å¥å­
        - æ–‡æœ¬è¶Šé•·ï¼Œåˆ†æè¶Šæº–ç¢º
        """)
        
        st.divider()
        st.caption("ğŸ“… 2025å¹´12æœˆ14æ—¥")
        st.caption("ğŸ”— [GitHub Repository](https://github.com)")
    
    # --- ä¸»è¦å…§å®¹å€ ---
    col1, col2 = st.columns([1, 1])
    
    # å®šç¾©ç¯„ä¾‹æ–‡æœ¬
    AI_EXAMPLE = "Artificial Intelligence represents a transformative branch of computer science. Furthermore, it aims to create intelligent machines capable of performing complex tasks. Moreover, these systems can analyze vast amounts of data efficiently. Consequently, AI has become increasingly important in modern technology."
    HUMAN_EXAMPLE = "I totally messed up the meeting today! Omg, my cat literally jumped on the keyboard right in the middle of my presentation. So embarrassing. But hey, at least everyone laughed? Sometimes life just throws curveballs at you, ya know?"
    
    with col1:
        # ç¯„ä¾‹æŒ‰éˆ• - ä½¿ç”¨ callback ç¢ºä¿æ­£ç¢ºæ›´æ–°
        st.markdown("**ğŸ“ å¿«é€Ÿæ¸¬è©¦ç¯„ä¾‹ï¼š**")
        b_col1, b_col2 = st.columns(2)
        
        def set_ai_example():
            st.session_state.text_input = AI_EXAMPLE
        
        def set_human_example():
            st.session_state.text_input = HUMAN_EXAMPLE
        
        with b_col1:
            st.button("ğŸ¤– AI ç”Ÿæˆç¯„ä¾‹", use_container_width=True, on_click=set_ai_example)
        with b_col2:
            st.button("âœï¸ äººé¡æ’°å¯«ç¯„ä¾‹", use_container_width=True, on_click=set_human_example)
        
        # æ–‡å­—æ¡† - ä½¿ç”¨ key ç¶å®šåˆ° session_state
        input_text = st.text_area(
            "è«‹è¼¸å…¥è‹±æ–‡æ–‡æœ¬",
            height=300,
            placeholder="Paste English text here to analyze...",
            help="æ”¯æ´è‡³å°‘ 2-3 å€‹å¥å­çš„è‹±æ–‡æ–‡æœ¬",
            key="text_input"
        )

    # --- åˆ†ææŒ‰éˆ• ---
    if st.button("ğŸš€ é–‹å§‹åˆ†æ", type="primary", use_container_width=True):
        if not input_text.strip():
            st.error("âš ï¸ è«‹è¼¸å…¥æ–‡å­—ï¼")
            return
        
        if len(input_text.split()) < 10:
            st.warning("âš ï¸ æ–‡å­—éçŸ­ï¼Œåˆ†æçµæœå¯èƒ½ä¸æº–ç¢ºã€‚å»ºè­°è‡³å°‘è¼¸å…¥ 10 å€‹å–®å­—ã€‚")
        
        try:
            with st.spinner("ğŸ” åˆ†æä¸­..."):
                # è¨ˆç®—æŒ‡æ¨™
                pp, words, word_surprises = calculate_perplexity(input_text)
                burstiness, avg_len, sentence_lengths = calculate_burstiness(input_text)
                confidence = calculate_confidence(pp, burstiness)
                is_ai_likely = confidence > 50
            
            # --- çµæœé¡¯ç¤º ---
            with col2:
                st.subheader("ğŸ“Š åˆ†æçµæœ")
                
                if is_ai_likely:
                    st.error(f"ğŸ¤– åˆ¤å®šçµæœï¼šé«˜åº¦ç–‘ä¼¼ AI ç”Ÿæˆ\n\n**AI å¯èƒ½æ€§ï¼š{confidence:.1f}%**")
                else:
                    st.success(f"ğŸ§‘ åˆ¤å®šçµæœï¼šé«˜åº¦ç–‘ä¼¼äººé¡æ’°å¯«\n\n**äººé¡å¯èƒ½æ€§ï¼š{100-confidence:.1f}%**")
                
                # æŒ‡æ¨™å¡ç‰‡
                m1, m2, m3 = st.columns(3)
                with m1:
                    st.metric(
                        "Perplexity",
                        f"{pp:.1f}",
                        "AI" if pp < 60 else "Human",
                        delta_color="inverse"
                    )
                with m2:
                    st.metric(
                        "Burstiness",
                        f"{burstiness:.2f}",
                        "AI" if burstiness < 0.5 else "Human",
                        delta_color="inverse"
                    )
                with m3:
                    st.metric(
                        "å¥å­æ•¸",
                        len(sentence_lengths),
                        f"å¹³å‡ {avg_len:.1f} è©"
                    )
                
                st.caption("ğŸ’¡ **åŸç†**ï¼šAI æ–‡æœ¬é€šå¸¸æ›´å¹³æ»‘ï¼ˆä½ Perplexityï¼‰ä¸”å¥é•·ç©©å®šï¼ˆä½ Burstinessï¼‰")
                
                # --- åŒ¯å‡ºå ±å‘Š ---
                st.divider()
                verdict = "AI ç”Ÿæˆ" if is_ai_likely else "äººé¡æ’°å¯«"
                word_count = len(words)
                unique_words = len(set(words))
                
                report = f"""AI æ–‡æœ¬åµæ¸¬å ±å‘Š
{'='*60}

ğŸ“ åˆ†ææ–‡æœ¬ï¼š
{input_text[:300]}{'...' if len(input_text) > 300 else ''}

ğŸ“Š çµ±è¨ˆè³‡è¨Šï¼š
â€¢ ç¸½è©æ•¸ï¼š{word_count}
â€¢ ä¸é‡è¤‡è©æ•¸ï¼š{unique_words}
â€¢ è©å½™è±å¯Œåº¦ï¼š{unique_words/word_count:.2%}
â€¢ å¥å­æ•¸é‡ï¼š{len(sentence_lengths)}
â€¢ å¹³å‡å¥é•·ï¼š{avg_len:.1f} è©

ğŸ“ˆ åˆ†ææŒ‡æ¨™ï¼š
â€¢ Perplexity (å›°æƒ‘åº¦)ï¼š{pp:.2f}
  â”” è§£é‡‹ï¼š{'æ•¸å€¼è¼ƒä½ï¼Œé¡¯ç¤ºæ–‡æœ¬å¯é æ¸¬æ€§é«˜ (AI ç‰¹å¾µ)' if pp < 60 else 'æ•¸å€¼è¼ƒé«˜ï¼Œé¡¯ç¤ºæ–‡æœ¬ä¸å¯é æ¸¬æ€§å¼· (äººé¡ç‰¹å¾µ)'}

â€¢ Burstiness (å¥å­ç¯€å¥)ï¼š{burstiness:.2f}
  â”” è§£é‡‹ï¼š{'å¥é•·è®ŠåŒ–å°ï¼Œç¯€å¥ç©©å®š (AI ç‰¹å¾µ)' if burstiness < 0.5 else 'å¥é•·è®ŠåŒ–å¤§ï¼Œç¯€å¥å¤šè®Š (äººé¡ç‰¹å¾µ)'}

ğŸ¯ åˆ¤å®šçµæœï¼š
{verdict} (å¯èƒ½æ€§ {confidence:.1f}%)

ğŸ’¡ åˆ†æèªªæ˜ï¼š
{'AI æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—é€šå¸¸è¡¨ç¾å‡ºä»¥ä¸‹ç‰¹å¾µï¼š' if is_ai_likely else 'äººé¡æ’°å¯«çš„æ–‡å­—é€šå¸¸è¡¨ç¾å‡ºä»¥ä¸‹ç‰¹å¾µï¼š'}
{'â€¢ ä½¿ç”¨å¤§é‡å¸¸è¦‹è©å½™å’Œå­¸è¡“è½‰æŠ˜è©' if is_ai_likely else 'â€¢ è©å½™é¸æ“‡æ›´åŠ å¤šæ¨£åŒ–å’Œå€‹æ€§åŒ–'}
{'â€¢ å¥å­é•·åº¦åˆ†ä½ˆå‡å‹»ï¼Œç¼ºä¹ç¯€å¥è®ŠåŒ–' if is_ai_likely else 'â€¢ å¥å­é•·åº¦è®ŠåŒ–å¤§ï¼Œæœ‰é•·çŸ­äº¤éŒ¯çš„ç¯€å¥'}
{'â€¢ æ–‡å­—æµæš¢ä½†ç¼ºä¹æƒ…æ„Ÿæ³¢å‹•' if is_ai_likely else 'â€¢ åŒ…å«å£èªåŒ–è¡¨é”ã€æ„Ÿå˜†è©æˆ–æƒ…ç·’ç”¨èª'}

âš ï¸ æ³¨æ„äº‹é …ï¼š
æœ¬åˆ†æåŸºæ–¼å•Ÿç™¼å¼ç®—æ³•ï¼Œçµåˆå¤šå€‹æ–‡æœ¬ç‰¹å¾µé€²è¡Œç¶œåˆåˆ¤æ–·ã€‚
å»ºè­°è¼¸å…¥è‡³å°‘ 50 è©ä»¥ç²å¾—æ›´æº–ç¢ºçš„åˆ†æçµæœã€‚

{'='*60}
ç”Ÿæˆæ™‚é–“ï¼š2025å¹´12æœˆ14æ—¥
åµæ¸¬å¼•æ“ï¼šAI Text Detector v1.0
"""
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰åˆ†æå ±å‘Š (.txt)",
                    data=report,
                    file_name=f"ai_detection_report_pp{pp:.0f}_burst{burstiness:.2f}.txt",
                    mime="text/plain",
                    use_container_width=True
                )
            
            # --- è¦–è¦ºåŒ–å€åŸŸ ---
            st.divider()
            st.subheader("ğŸ” è©³ç´°åˆ†æè¦–è¦ºåŒ–")
            
            tab1, tab2 = st.tabs(["ğŸ“Š å¥å­é•·åº¦åˆ†ä½ˆ", "ğŸ¨ è©å½™ç†±åŠ›åœ–"])
            
            with tab1:
                st.write("**å¥å­é•·åº¦è®ŠåŒ– (Burstiness Visualization)**")
                if len(sentence_lengths) > 0:
                    st.bar_chart(sentence_lengths)
                    st.caption(f"æ¨™æº–å·®ï¼š{np.std(sentence_lengths):.2f} | å¹³å‡å€¼ï¼š{avg_len:.1f} | Burstinessï¼š{burstiness:.2f}")
                    st.caption("ğŸ’¡ AI æ–‡æœ¬çš„å¥é•·é€šå¸¸è¼ƒç‚ºå¹³å‡ï¼Œäººé¡æ–‡æœ¬å‰‡æœ‰æ˜é¡¯çš„é•·çŸ­å¥äº¤éŒ¯ã€‚")
            
            with tab2:
                st.write("**è©å½™é©šå–œåº¦ç†±åŠ›åœ– (Perplexity Heatmap)**")
                st.info("ğŸŸ¥ ç´…è‰² = å¸¸è¦‹è©/ä½é©šå–œåº¦ (AI ç‰¹å¾µ) | ğŸŸ© ç¶ è‰² = ç½•è¦‹è©/é«˜é©šå–œåº¦ (äººé¡ç‰¹å¾µ)")
                
                # ç”Ÿæˆç†±åŠ›åœ–
                html_code = "<div style='line-height: 2.2; font-family: monospace; font-size: 15px; padding: 15px;'>"
                
                max_surprise = np.percentile(word_surprises, 90) if len(word_surprises) > 0 else 5
                
                for word, surprise in zip(words, word_surprises):
                    normalized = min(surprise / max_surprise, 1)
                    
                    if normalized < 0.4:
                        bg_color = f"rgba(255, 0, 0, {0.6 - normalized})"
                        border_color = "rgba(255, 0, 0, 0.8)"
                    else:
                        bg_color = f"rgba(0, 200, 0, {(normalized - 0.4) * 0.8})"
                        border_color = "rgba(0, 200, 0, 0.8)"
                    
                    html_code += f"<span style='background-color: {bg_color}; padding: 3px 6px; margin: 2px; border-radius: 4px; border-bottom: 2px solid {border_color}; display: inline-block;'>{word}</span>"
                
                html_code += "</div>"
                st.markdown(html_code, unsafe_allow_html=True)
                st.caption("ğŸ’¡ ç´…è‰²å€åŸŸè¡¨ç¤ºæ¨¡å‹å®¹æ˜“é æ¸¬çš„è©å½™ï¼ˆå¸¸è¦‹æ–¼ AI æ–‡æœ¬ï¼‰ï¼Œç¶ è‰²å€åŸŸè¡¨ç¤ºæ„å¤–çš„è©å½™ï¼ˆå¸¸è¦‹æ–¼äººé¡æ–‡æœ¬ï¼‰ã€‚")
        
        except Exception as e:
            st.error(f"âŒ åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            import traceback
            st.code(traceback.format_exc())
            st.info("ğŸ’¡ è«‹æª¢æŸ¥æ–‡å­—æ ¼å¼ï¼Œç¢ºä¿æ˜¯æœ‰æ•ˆçš„è‹±æ–‡æ–‡æœ¬ã€‚")

if __name__ == "__main__":
    main()
